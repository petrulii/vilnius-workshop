{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on Bilevel Optimization\n",
    "### with a linear deterministic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add main project directory path\n",
    "sys.path.append('./src')\n",
    "\n",
    "from model.FunctionApproximator.FunctionApproximator import FunctionApproximator\n",
    "from model.BilevelProblem.BilevelProblem import BilevelProblem\n",
    "from model.NeuralNetworks.NeuralNetworkOuterModel import NeuralNetworkOuterModel\n",
    "from model.utils import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "set_seed(seed=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the dimensions and generate the data, in this toy example we have linear data where values $y \\in R^{m\\times 1}$ are generated by the linear function $h^*(w)=X\\theta$ parametrized by the true coefficient vector $\\theta\\in R^{n\\times 1}$ with the feature matrix $X\\in R^{m\\times n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dimesnions\n",
    "n, m, m_v, m_t, batch = 2, 1000, 300, 700, 64\n",
    "# The coefficient tensor of size (n,1) filled with values uniformally sampled from the range (0,1)\n",
    "coef = np.random.uniform(size=(n,1)).astype('float32')\n",
    "# The data tensor of size (m,n) filled with values uniformally sampled from the range (0,1)\n",
    "X = np.random.uniform(size=(m, n)).astype('float32')\n",
    "# True h_star\n",
    "h_true = lambda X: X @ coef\n",
    "y = h_true(X)\n",
    "# Split X into 2 tensors with sizes [m_t, m_v] along dimension 0\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# Convert everything to PyTorch tensors\n",
    "X_train, X_val, y_train, y_val, coef = torch.from_numpy(X_train), torch.from_numpy(X_val), torch.from_numpy(y_train), torch.from_numpy(y_val), torch.from_numpy(coef)\n",
    "print(\"True coeficients:\", coef)\n",
    "print(\"X training data:\", X_train[1:5])\n",
    "print(\"y training labels:\", y_train[1:5])\n",
    "print()\n",
    "\n",
    "dataset = [X_val,y_val,X_train,y_train]\n",
    "maxiter = 1000\n",
    "step = 0.1\n",
    "mu_0_value = 3.\n",
    "mu0 = torch.full((1,1), mu_0_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4.5,4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_train[:,0], X_train[:,1], y_train, marker='.')\n",
    "ax.set_xlabel(''r'$x_1$', fontsize=12)\n",
    "ax.set_ylabel(''r'$x_2$', fontsize=12)\n",
    "ax.set_zlabel(''r'$h(x)$', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Implicit Differentiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider a linear toy example of a bilevel problem where $X_v$ is a $m_v\\times n$ **validation set** feature matrix (every row is a feature vector of dimension $n$) and $y_v$ is an $m_v$ dimensional **validation set** true value vector (every member is the true value for a corresponding feature vector). Similarly, $X_t \\in R^{m_t\\times n}$ and $y \\in R^{m_t}$ constitute the **training set**. We have the outer objective:\n",
    "$$\n",
    "  F(\\mu, \\theta^*) = \\tfrac{1}{2}\\lVert X_v\\theta^* - y_v \\lVert^2\n",
    "$$\n",
    "and the inner objective:\n",
    "$$\n",
    "  s.t.\\ \\theta^*(x)\\in \\arg \\min_{\\theta} G(\\mu, \\theta) = \\tfrac{1}{2}\\lVert X_t\\theta - y_t \\lVert^2 + \\mu\\lVert X_t\\theta \\lVert^2\n",
    "$$\n",
    "To use classical implicit differentiation we give a closed form solution of the lower objective $G(\\mu, \\theta)$ for a fixed $\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to find optimal or closed form solution of h*\n",
    "def find_theta_star(X, y, mu):\n",
    "    \"\"\"\n",
    "    Find a closed form solution of theta for a fixed mu.\n",
    "    \"\"\"\n",
    "    return torch.linalg.inv((1+2*mu) * X.T @ X) @ X.T @ y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then provide all necessary gradients to compute the gradient $\\nabla_\\mu F$ as functions of the parameter vector $\\theta$ and the outer parameter $\\mu$:\n",
    "> Fill the inner objective function `fi` for *classical implicit differentiation* in the cell below, it should depend on $\\theta$ as given in the mathematical expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective functions\n",
    "fo = lambda mu, theta, X_out, y_out: torch.mean((1/2)*torch.pow(((X_out @ theta) - y_out),2))\n",
    "fi = lambda mu, theta, X_in, y_in: 0 # !!! TO FILL !!!\n",
    "\n",
    "# Gradients\n",
    "og1 = lambda mu, theta, X_out, y_out: torch.tensor([[0]])\n",
    "og2 = lambda mu, theta, X_out, y_out: X_out.T @ (X_out @ theta - y_out)\n",
    "ig22 = lambda mu, theta, X_in, y_in: X_in.T @ X_in + 2 * mu * X_in.T @ X_in\n",
    "ig12 = lambda mu, theta, X_in, y_in: 2 * X_in.T @ X_in @ theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use **classical implicit differentiation** to find the optimal $\\mu^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize using classical implicit differention\n",
    "bp_classic = BilevelProblem(outer_objective=fo, inner_objective=fi, method=\"implicit_diff\", data=dataset, gradients=[og1,og2,ig22,ig12], find_theta_star=find_theta_star)\n",
    "mu_opt_c, iters_c, n_iters_c, times, inner_loss, outer_loss, theta = bp_classic.optimize(mu0, maxiter=maxiter, step=step)\n",
    "plot_loss(\"Inner loss clas.\", inner_loss, title=\"Inner loss of clas. im. diff.\")\n",
    "plot_loss(\"Outer loss clas.\", outer_loss, title=\"Outer loss of clas. im. diff.\")\n",
    "h_star_c = lambda X: X @ theta\n",
    "\n",
    "# Show results\n",
    "print(\"CLASSICAL IMPLICIT DIFFERENTIATION\")\n",
    "print(\"Argmin of the outer objective:\", mu_opt_c)\n",
    "print(\"Average iteration time:\", np.average(times))\n",
    "print(\"Number of iterations:\", n_iters_c)\n",
    "print(\"True coefficients theta:\", coef)\n",
    "print(\"Fitted coefficients theta:\", theta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Implicit Differentiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use neural implicit differentiation we reformulate the problem into a **functional bilevel problem**:\n",
    "$$\n",
    "  F(\\mu, h^*) = \\tfrac{1}{2} \\lVert h^*(X_v) - y_v \\lVert^2\n",
    "$$\n",
    "and the inner objective:\n",
    "$$\n",
    "  s.t.\\ h^*(x)\\in \\arg \\min_{h\\in H} G(\\mu, h) = \\tfrac{1}{2} \\lVert h(X_t) - y_t \\lVert^2 + \\mu\\lVert h(X_t) \\lVert^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide all necessary gradients to compute the gradient $\\nabla_\\mu F$ as functions of the function $h$ and the outer parameter $\\mu$:\n",
    "> Fill the inner objective function `fi` for *neural implicit differentiation* in the cell below, it should depend on the function $h$ as given in the mathematical expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective functions\n",
    "fo = lambda mu, h, X_out, y_out: torch.mean((1/2)*torch.pow((h(X_out) - y_out),2))\n",
    "fi = lambda mu, h, X_in, y_in: 0 # !!! TO FILL !!!\n",
    "\n",
    "# Gradients\n",
    "og1 = lambda mu, h, X_out, y_out: torch.tensor([[0]])\n",
    "og2 = lambda mu, h, X_out, y_out: (h(X_out) - y_out)\n",
    "ig22 = lambda mu, h, X_in, y_in: torch.eye(len(y_in)) * (1+2*mu)\n",
    "ig12 = lambda mu, h, X_in, y_in: 2*h(X_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use **neural implicit differentiation** to find the optimal $\\mu^*$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize using neural implicit differention\n",
    "bp_neural = BilevelProblem(outer_objective=fo, inner_objective=fi, fo_h_X=fo, fi_h_X=fi, method=\"neural_implicit_diff\", data=dataset, gradients=[og1,og2,ig22,ig12])\n",
    "mu_opt_n, iters_n, n_iters_n, times, inner_loss, outer_loss, h_star_n = bp_neural.optimize(mu0, maxiter=maxiter, step=step)\n",
    "plot_loss(\"Inner loss neur.\", inner_loss, title=\"Inner loss of neur. im. diff.\")\n",
    "plot_loss(\"Outer loss neur.\", outer_loss, title=\"Outer loss of neur. im. diff.\")\n",
    "\n",
    "# Show results\n",
    "print(\"NEURAL IMPLICIT DIFFERENTIATION\")\n",
    "print(\"Argmin of the outer objective:\", mu_opt_n)\n",
    "print(\"Average iteration time:\", np.average(times))\n",
    "print(\"Number of iterations:\", n_iters_n)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camparison\n",
    "We can compare classical and neural implicit differentiation by looking at the prediction function $h$ that the model has inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2D_functions(\"2Dfunctions\", h_true, h_star_c, h_star_n, points=None, plot_x_lim=[0,mu_0_value], plot_y_lim=[0,1], plot_nb_contours=80, titles=[\"True Imp. Diff.\",\"Classical Imp. Diff.\",\"Neural Imp. Diff.\"])\n",
    "plot_1D_iterations(\"1Dfunctions\", [i for i in range(n_iters_c)], [i for i in range(n_iters_n)], [iters_c[i][0,0] for i in range(n_iters_c)], [iters_n[i][0,0] for i in range(n_iters_n)], plot_x_lim=[0,mu_0_value], titles=[\"Classical Imp. Diff.\",\"Neural Imp. Diff.\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also by looking at the trajectory of the iterations and how fast the outer variable $\\mu$ approaches the minimum $0$ in both cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
